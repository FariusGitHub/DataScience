{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stackoverflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7xSUy1uGhUgj+KH/AnBL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FariusGitHub/DataScience/blob/master/stackoverflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud3a-DtiGBhz"
      },
      "source": [
        "# Elasticsearch for Stackoverflow Knowledge Base\n",
        "Farius Tjioesman, April 29, 2021 - ftjioesman@yahoo.com\n",
        "\n",
        "Following my former colleague work and suggestion from big data expert I continue the <br>  knowledge base on similiarity analysis with indexing and querying from ElasticSearch <br> \n",
        "that is using BM25 by default (almost similar to traditional TF-IDF). Based on cleaned <br> Parquet data from my colleague he has almost half a million row data left (482,774).<br> \n",
        "\n",
        "The elasticseach score was derived from the multiplication of three components <br> Boost factor, idf factor and tf factor. Boost factor is 2.2 by default.\n",
        "\n",
        "Herewith is the formula for tf\n",
        "\n",
        "![Untitled.png](data:image/png;base64,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)\n",
        "\n",
        "And herewith is the formula for idf <br>\n",
        "![Untitled2.png](data:image/png;base64,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)\n",
        "\n",
        "See article below from Anirudh Dey on August 2020 for above legends<br>\n",
        "https://codeburst.io/deconstructing-scoring-in-elasticsearch-e8544676a24\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9DjQGf7HeIs"
      },
      "source": [
        "from time import sleep\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DldJV1kPHkSX",
        "outputId": "1d32950f-3493-4cb1-f31a-383e26ac2a9d"
      },
      "source": [
        "# install es server\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q --show-progress\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elasticsearch-7.0.0 100%[===================>] 330.70M  37.8MB/s    in 8.2s    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qDVbseqI2gu"
      },
      "source": [
        "# The server may take some time prior to get response from Virtual Environment\n",
        "sleep(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dexw5kIHn65",
        "outputId": "483a616f-dcc1-458e-98ca-af6ad406b12d"
      },
      "source": [
        "# start server and check es virtual machine connection\n",
        "es_server = Popen(['elasticsearch-7.0.0/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )\n",
        "# wait a bit then test, if failed than re-run this cell\n",
        "!curl -X GET \"localhost:9200/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\" : \"ceddcb8480d3\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"wDpkePIHQ5Cdcdd1CszYIg\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.0.0\",\n",
            "    \"build_flavor\" : \"default\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"b7e28a7\",\n",
            "    \"build_date\" : \"2019-04-05T22:55:32.697037Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.0.0\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.7.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccrycfx6HzZ3",
        "outputId": "553a720b-0ddb-4aa8-ce6a-d3fe9a4c7e11"
      },
      "source": [
        "# client-side\n",
        "!pip install elasticsearch -q\n",
        "# Import Elasticsearch package\n",
        "from elasticsearch import Elasticsearch \n",
        "es = Elasticsearch()\n",
        "es.ping()  # got True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059PWBzP9gO8"
      },
      "source": [
        "#Extract cleaned data \n",
        "docs=pd.read_csv('https://farius.s3.us-east-2.amazonaws.com/EV3/bigdata/Dakshin_482774.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KbCS3UgpC5lc",
        "outputId": "76a99eb9-b986-4110-c587-88f4c3e35222"
      },
      "source": [
        "docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_Id</th>\n",
              "      <th>text</th>\n",
              "      <th>L1_py</th>\n",
              "      <th>L1_ML</th>\n",
              "      <th>L1_DL</th>\n",
              "      <th>L1_BD</th>\n",
              "      <th>L1_SQL</th>\n",
              "      <th>L1_O</th>\n",
              "      <th>L2_scikit-learn</th>\n",
              "      <th>L2_postgresql</th>\n",
              "      <th>L2_c#</th>\n",
              "      <th>L2_tensorflow</th>\n",
              "      <th>L2_numpy</th>\n",
              "      <th>L2_database</th>\n",
              "      <th>L2_pandas</th>\n",
              "      <th>L2_mysql</th>\n",
              "      <th>L2_docker</th>\n",
              "      <th>L2_keras</th>\n",
              "      <th>L2_bash</th>\n",
              "      <th>L2_sql-server</th>\n",
              "      <th>L2_nltk</th>\n",
              "      <th>L2_pyspark</th>\n",
              "      <th>L2_matplotlib</th>\n",
              "      <th>L2_lstm</th>\n",
              "      <th>L2_c</th>\n",
              "      <th>L2_shell</th>\n",
              "      <th>L2_list</th>\n",
              "      <th>L2_neural-network</th>\n",
              "      <th>L2_linux</th>\n",
              "      <th>L2_apache-spark</th>\n",
              "      <th>L2_conv-neural-network</th>\n",
              "      <th>L2_nlp</th>\n",
              "      <th>L2_php</th>\n",
              "      <th>L2_django</th>\n",
              "      <th>L2_hadoop</th>\n",
              "      <th>L2_hive</th>\n",
              "      <th>L2_other</th>\n",
              "      <th>L1_py_weight</th>\n",
              "      <th>L1_ML_weight</th>\n",
              "      <th>L1_DL_weight</th>\n",
              "      <th>L1_BD_weight</th>\n",
              "      <th>L1_SQL_weight</th>\n",
              "      <th>L1_O_weight</th>\n",
              "      <th>L2_scikit-learn_weight</th>\n",
              "      <th>L2_postgresql_weight</th>\n",
              "      <th>L2_c#_weight</th>\n",
              "      <th>L2_tensorflow_weight</th>\n",
              "      <th>L2_numpy_weight</th>\n",
              "      <th>L2_database_weight</th>\n",
              "      <th>L2_pandas_weight</th>\n",
              "      <th>L2_mysql_weight</th>\n",
              "      <th>L2_docker_weight</th>\n",
              "      <th>L2_keras_weight</th>\n",
              "      <th>L2_bash_weight</th>\n",
              "      <th>L2_sql-server_weight</th>\n",
              "      <th>L2_nltk_weight</th>\n",
              "      <th>L2_pyspark_weight</th>\n",
              "      <th>L2_matplotlib_weight</th>\n",
              "      <th>L2_lstm_weight</th>\n",
              "      <th>L2_c_weight</th>\n",
              "      <th>L2_shell_weight</th>\n",
              "      <th>L2_list_weight</th>\n",
              "      <th>L2_neural-network_weight</th>\n",
              "      <th>L2_linux_weight</th>\n",
              "      <th>L2_apache-spark_weight</th>\n",
              "      <th>L2_conv-neural-network_weight</th>\n",
              "      <th>L2_nlp_weight</th>\n",
              "      <th>L2_php_weight</th>\n",
              "      <th>L2_django_weight</th>\n",
              "      <th>L2_hadoop_weight</th>\n",
              "      <th>L2_hive_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>62073693</td>\n",
              "      <td>trying wrtie two txtfiles test txt test txt cs...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.796387</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62073724</td>\n",
              "      <td>train model using linear regression used print...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.859452</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62073806</td>\n",
              "      <td>completely preprocess mnist dataset feeding cn...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.859452</td>\n",
              "      <td>0.870633</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62073823</td>\n",
              "      <td>webratio javhttps registry hub docker com r we...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62074040</td>\n",
              "      <td>following dataframe df index df date df groupb...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.796387</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482769</th>\n",
              "      <td>104983</td>\n",
              "      <td>python specifically variables get shared threa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.796387</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482770</th>\n",
              "      <td>105212</td>\n",
              "      <td>suppose directory dir inside symlinks director...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482771</th>\n",
              "      <td>105546</td>\n",
              "      <td>got loads options php mysql apache combo best ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.699984</td>\n",
              "      <td>0.799345</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482772</th>\n",
              "      <td>106850</td>\n",
              "      <td>trying write console terminal gaming console p...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.796387</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482773</th>\n",
              "      <td>108251</td>\n",
              "      <td>vps server fedora mingetty keeps respawning pe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.203613</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.129367</td>\n",
              "      <td>0.300016</td>\n",
              "      <td>0.200655</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.789485</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "      <td>0.210515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>482774 rows  71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             _Id  ... L2_hive_weight\n",
              "0       62073693  ...       0.210515\n",
              "1       62073724  ...       0.210515\n",
              "2       62073806  ...       0.210515\n",
              "3       62073823  ...       0.210515\n",
              "4       62074040  ...       0.210515\n",
              "...          ...  ...            ...\n",
              "482769    104983  ...       0.210515\n",
              "482770    105212  ...       0.210515\n",
              "482771    105546  ...       0.210515\n",
              "482772    106850  ...       0.210515\n",
              "482773    108251  ...       0.210515\n",
              "\n",
              "[482774 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urSUmmcoIjgx",
        "outputId": "ff0f8b6d-1a8b-42dc-f217-7d683a8b5429"
      },
      "source": [
        "#Ingesting documents into Elasticsearch at Google Colab (take 90 minutes)\n",
        "for i in tqdm(range(len(docs))):\n",
        "    \n",
        "    request_body = {\n",
        "    \"settings\": {\n",
        "        \"number_of_shards\": 5,\n",
        "        \"number_of_replicas\": 1\n",
        "    },\n",
        "    'mappings': {\n",
        "        'examplecase': {\n",
        "            'properties': {\n",
        "                'tbl_id': {'type': 'keyword'},\n",
        "                'texts': {'type': 'text'},\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'id' : docs.iloc[i][0],\n",
        "    'text_entry' : docs.iloc[i][1]\n",
        "    }\n",
        "    \n",
        "    es.index(index='stackoverflow', doc_type='Blog', id=i+1, body=request_body)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/482774 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n",
            "100%|| 482774/482774 [1:27:36<00:00, 91.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5niCF9YLXda",
        "outputId": "a424bdd0-ae81-4518-fc22-a707974a184a"
      },
      "source": [
        "# Checking data integrity\n",
        "!curl -X GET 'http://localhost:9200/stackoverflow/_count?pretty'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"count\" : 482774,\n",
            "  \"_shards\" : {\n",
            "    \"total\" : 1,\n",
            "    \"successful\" : 1,\n",
            "    \"skipped\" : 0,\n",
            "    \"failed\" : 0\n",
            "  }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoXJmMxPu8UX"
      },
      "source": [
        "Using .search and enabling explaination (explain=True) we can follow along <br> how _score was computed\n",
        "\n",
        "Reference : <br>\n",
        "https://marcobonzanini.com/2015/02/02/how-to-query-elasticsearch-with-python/ <br>\n",
        "https://www.mathworks.com/help///textanalytics/ref/bagofwords.tfidf.html <br>\n",
        "https://en.wikipedia.org/wiki/Okapi_BM25\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFs_fgizL7eF",
        "outputId": "b7c75bbd-5682-4f76-d98a-a53823333d73"
      },
      "source": [
        "query_word='elasticsearch instance databricks spark'\n",
        "#Display 10 results from a query with query_word above\n",
        "result=es.search(index=\"stackoverflow\", explain=True, size=10, doc_type=\"Blog\",\\\n",
        "                 body={\"query\": {\"match\": {\"text_entry\": query_word}}})\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in search requests is deprecated.\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 1, 'total': 1},\n",
              " 'hits': {'hits': [{'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 40055) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=4.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 4.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.80698806}],\n",
              "         'value': 10.732946}],\n",
              "       'value': 10.732946},\n",
              "      {'description': 'weight(text_entry:instance in 40055) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5110638}],\n",
              "         'value': 3.7518573}],\n",
              "       'value': 3.7518573},\n",
              "      {'description': 'weight(text_entry:databricks in 40055) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5110638}],\n",
              "         'value': 6.997923}],\n",
              "       'value': 6.997923},\n",
              "      {'description': 'weight(text_entry:spark in 40055) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5110638}],\n",
              "         'value': 3.9166842}],\n",
              "       'value': 3.9166842}],\n",
              "     'value': 25.39941},\n",
              "    '_id': '42157',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 25.39941,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 59295497,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'trying index data elasticsearch databricks using pyspark works configuration scala trying write es using pyspark fails needed based transforms using python packages esconf esconf es nodes wan true esconf es port esconf es net ssl true esconf es read metadata true esconf es nodes esurl esconf ex index auto create true esconf es resource auto create geohashing testing geocordsschema write format org elasticsearch spark sql options esconf mode overwrite save geohash test error produced org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting apos es nodes wan apos ideas suggestions missing config would work scala using pyspark'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 11619) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=6.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 6.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 112.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.8562599}],\n",
              "         'value': 11.388262}],\n",
              "       'value': 11.388262},\n",
              "      {'description': 'weight(text_entry:databricks in 11619) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 112.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.49820188}],\n",
              "         'value': 6.8218064}],\n",
              "       'value': 6.8218064},\n",
              "      {'description': 'weight(text_entry:spark in 11619) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=4.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 4.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 112.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.7988467}],\n",
              "         'value': 6.122191}],\n",
              "       'value': 6.122191}],\n",
              "     'value': 24.33226},\n",
              "    '_id': '278687',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 24.33226,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 39559121,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'figure write dataframe elasticsearch using python spark followed steps code read file df sqlcontext read format com databricks spark csv options header true load vagrant data input input csv schema customschema df registertemptable data kpis kpi sqlcontext sql select data es conf es nodes es port es resource kpi kpi rdd saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es conf code gives caused net razorvine pickle pickleexception expected zero arguments construction classdict pyspark sql types create row also started script spark submit master spark aggregator jars jars elasticsearch hadoop dist elasticsearch hadoop jar vagrant scripts aggregation py ensure elasticsearch hadoop loaded'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 54257) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=7.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 7.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 152.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.8474785}],\n",
              "         'value': 11.271469}],\n",
              "       'value': 11.271469},\n",
              "      {'description': 'weight(text_entry:instance in 54257) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=2.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 2.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 152.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.6135352}],\n",
              "         'value': 4.5041275}],\n",
              "       'value': 4.5041275},\n",
              "      {'description': 'weight(text_entry:spark in 54257) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=4.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 4.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 152.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.7604857}],\n",
              "         'value': 5.8282003}],\n",
              "       'value': 5.8282003}],\n",
              "     'value': 21.603798},\n",
              "    '_id': '56359',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 21.603798,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 61453492,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'running spark inside glue write aws elasticsearch following configuration spark conf set es nodes nodes indexname conf set es port conf set es batch write retry count conf set es batch size bytes kb conf set es batch size entries conf set es index auto create false conf set es nodes wan true conf set es net ssl true however get following error diagnostics user class threw exception org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan org elasticsearch hadoop rest initializationutils discoverclusterinfo initializationutils java org elasticsearch spark rdd esspark dosavetoes esspark scala know vpc running elasticsearch instance sure set glue spark different problem idea also tried add glue jdbc connection use proper vpc connection sure set properly import scala reflect runtime universe def savetoes lt product typetag index string data rdd sparkprovider gluecontext getjdbcsink catalogconnection elasticsearch connection options jsonoptions transformationcontext sinktoelasticsearch writedynamicframe dynamicframe sparkprovider sqlcontext createdataframe data sparkprovider gluecontext'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:instance in 9037) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=3.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 3.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.758207}],\n",
              "         'value': 5.566202}],\n",
              "       'value': 5.566202},\n",
              "      {'description': 'weight(text_entry:databricks in 9037) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=2.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 2.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.67642915}],\n",
              "         'value': 9.262247}],\n",
              "       'value': 9.262247},\n",
              "      {'description': 'weight(text_entry:spark in 9037) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=5.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 5.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 104.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.8393905}],\n",
              "         'value': 6.43291}],\n",
              "       'value': 6.43291}],\n",
              "     'value': 21.26136},\n",
              "    '_id': '11139',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 21.26136,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 61713014,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'new big data infrastructure need refactor web service actually done python querying google bigquery aspnet mvc core querying azure databricks would like run sql query spark instance azure databricks actual implementation using pyspark sparksession found implementation library net net apache spark microsoft spark feel like app using must run close spark instance need install java proper spark instance really feel missing something expecting something similar database call connexion string api accessed web site web site allows user build scenario defining couple attributes run query using attribute call web api api return sql result data frame translated model could align right technologies much going azure big data thank much help'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:instance in 44979) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 136.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.46322775}],\n",
              "         'value': 3.4006798}],\n",
              "       'value': 3.4006798},\n",
              "      {'description': 'weight(text_entry:databricks in 44979) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=4.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 4.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 136.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.77537936}],\n",
              "         'value': 10.617158}],\n",
              "       'value': 10.617158},\n",
              "      {'description': 'weight(text_entry:spark in 44979) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=14.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 14.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 136.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.92355806}],\n",
              "         'value': 7.077952}],\n",
              "       'value': 7.077952}],\n",
              "     'value': 21.095789},\n",
              "    '_id': '312047',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 21.095789,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 35346567,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'spark cluster use local mode want read csv databricks external library spark csv start app follows import os import sys os environ spark home home mebuddy programs spark bin hadoop spark home os environ get spark home none sys path insert spark home python sys path insert os path join spark home python lib py j src zip pyspark import sparkcontext sparkconf sqlcontext try sc except nameerror print initializing sparkcontext sc sparkcontext sq sqlcontext sc df sq read format com databricks spark csv options header true inferschema true load path file csv run get following error java lang classnotfoundexception failed load class data source com databricks spark csv question load databricks spark csv library inside python code want load outside using packages instance tried add following lines work os environ spark classpath home mebuddy programs spark lib spark csv jar'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 18783) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=25.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 25.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 376.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.9033674}],\n",
              "         'value': 12.0147915}],\n",
              "       'value': 12.0147915},\n",
              "      {'description': 'weight(text_entry:instance in 18783) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 376.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.27216563}],\n",
              "         'value': 1.9980412}],\n",
              "       'value': 1.9980412},\n",
              "      {'description': 'weight(text_entry:spark in 18783) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=24.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 24.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 376.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.8997447}],\n",
              "         'value': 6.8954515}],\n",
              "       'value': 6.8954515}],\n",
              "     'value': 20.908283},\n",
              "    '_id': '20885',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 20.908283,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 60059911,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'trying read data elasticsearch via spark scala see lot post addressing question tried options mentioned various posts seems nothing working jar used elasticsearch hadoop jar used elasticsearch spark jar without success elastic search version spark scala code import org apache spark sparkconf import org apache spark sql sparksession import org apache spark sparkcontext import org apache spark sparkcontext import org elasticsearch spark val spark sparksession builder appname elasticspark master local getorcreate val reader spark read format org elasticsearch spark sql option es index auto create true option spark serializer org apache spark serializer kryoserializer option es port option es nodes xxxxxxxxx option es nodes wan true option es net http auth user xxxxxx option es net http auth pass xxxxxxxx val read reader load index type error error rest networkclient node xxxxxxxxx failed server xxxxxxxxxxxxx failed respond nodes left aborting org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan org elasticsearch hadoop rest initializationutils discoveresversion initializationutils java org elasticsearch spark sql schemautils discovermappingandgeofields schemautils scala org elasticsearch spark sql schemautils discovermapping schemautils scala org elasticsearch spark sql elasticsearchrelation lazyschema lzycompute defaultsource scala org elasticsearch spark sql elasticsearchrelation lazyschema defaultsource scala org elasticsearch spark sql elasticsearchrelation anonfun schema apply defaultsource scala org elasticsearch spark sql elasticsearchrelation anonfun schema apply defaultsource scala scala option getorelse option scala org elasticsearch spark sql elasticsearchrelation schema defaultsource scala org apache spark sql execution datasources datasource resolverelation datasource scala org apache spark sql dataframereader loadv source dataframereader scala org apache spark sql dataframereader load dataframereader scala org apache spark sql dataframereader load dataframereader scala elided caused org elasticsearch hadoop rest eshadoopnonodesleftexception connection error check network proxy settings nodes failed tried xxxxxxxxxxx org elasticsearch hadoop rest networkclient execute networkclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient get restclient java org elasticsearch hadoop rest restclient remoteesversion restclient java org elasticsearch hadoop rest initializationutils discoveresversion initializationutils java apart also tried properties without success option es net ssl cert allow self signed true option es net ssl truststore location lt path elasticsearch cert file gt option es net ssl truststore pass xxxxxx please note elasticsearch node within unix edge node http xxxxxx mentioning case makes difference code missing properties please help'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 108471) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=5.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 5.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.87655866}],\n",
              "         'value': 11.658236}],\n",
              "       'value': 11.658236},\n",
              "      {'description': 'weight(text_entry:instance in 108471) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5868115}],\n",
              "         'value': 4.3079414}],\n",
              "       'value': 4.3079414},\n",
              "      {'description': 'weight(text_entry:spark in 108471) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5868115}],\n",
              "         'value': 4.497198}],\n",
              "       'value': 4.497198}],\n",
              "     'value': 20.463375},\n",
              "    '_id': '110573',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 20.463375,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 55039308,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'trying loading data pyspark jupyteer notebook elasticsearch gives error error occurred calling z org apache spark api python pythonrdd saveasnewapihadoopfile org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan conf x saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat get pyspark jar file valueclass org elasticsearch hadoop mr linkedmapwritable conf conf'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:instance in 50096) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=2.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 2.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.73961085}],\n",
              "         'value': 5.4296827}],\n",
              "       'value': 5.4296827},\n",
              "      {'description': 'weight(text_entry:databricks in 50096) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.5868115}],\n",
              "         'value': 8.035125}],\n",
              "       'value': 8.035125},\n",
              "      {'description': 'weight(text_entry:spark in 50096) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=5.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 5.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 64.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.87655866}],\n",
              "         'value': 6.7177587}],\n",
              "       'value': 6.7177587}],\n",
              "     'value': 20.182566},\n",
              "    '_id': '183631',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 20.182566,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 45583607,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'use cloudera deploy zeppelin spark yarn hdfs cluster right one instance zeppelin spark execution spark notebooks affects every user instance stop spark context user notebook affects user notebooks seen option zeppelin isolate interpreters way provide user cluster demand maybe using docker building image zeppelin spark user limiting resources ones provided user cluster quite lost implement even possible ideal scenario would approach like databricks cluster resources isolated users'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:instance in 78723) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 17160},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.3369405},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 88.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.53888845}],\n",
              "         'value': 3.9561253}],\n",
              "       'value': 3.9561253},\n",
              "      {'description': 'weight(text_entry:databricks in 78723) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=2.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 2.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 88.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.70036066}],\n",
              "         'value': 9.589937}],\n",
              "       'value': 9.589937},\n",
              "      {'description': 'weight(text_entry:spark in 78723) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=5.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 5.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 88.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.853873}],\n",
              "         'value': 6.543901}],\n",
              "       'value': 6.543901}],\n",
              "     'value': 20.089964},\n",
              "    '_id': '80825',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 20.089964,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 56481295,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'need connect spark redshift instance generate data using spark scala used compatible jdbc connector spark redshift connector facing weird problem using pyspark df sqlcontext read format com databricks spark redshift option query select top fact table option url jdbc redshift redshift host events user usernmae amp password pass option tempdir redshift archive load df show gives error permission denied bucket weird see files created bucket read ps set accesskey secret access key also ps also confused n file system connector used https github com databricks spark redshift tree branch x'},\n",
              "    '_type': 'Blog'},\n",
              "   {'_explanation': {'description': 'sum of:',\n",
              "     'details': [{'description': 'weight(text_entry:elasticsearch in 121014) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=5.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 1143},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.045457},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 5.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 264.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.71766746}],\n",
              "         'value': 9.544981}],\n",
              "       'value': 9.544981},\n",
              "      {'description': 'weight(text_entry:databricks in 121014) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=1.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 956},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 6.2240252},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 1.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 264.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.33703908}],\n",
              "         'value': 4.6150274}],\n",
              "       'value': 4.6150274},\n",
              "      {'description': 'weight(text_entry:spark in 121014) [PerFieldSimilarity], result of:',\n",
              "       'details': [{'description': 'score(freq=6.0), product of:',\n",
              "         'details': [{'description': 'boost', 'details': [], 'value': 2.2},\n",
              "          {'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
              "           'details': [{'description': 'n, number of documents containing term',\n",
              "             'details': [],\n",
              "             'value': 14820},\n",
              "            {'description': 'N, total number of documents with field',\n",
              "             'details': [],\n",
              "             'value': 482774}],\n",
              "           'value': 3.4835393},\n",
              "          {'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
              "           'details': [{'description': 'freq, occurrences of term within document',\n",
              "             'details': [],\n",
              "             'value': 6.0},\n",
              "            {'description': 'k1, term saturation parameter',\n",
              "             'details': [],\n",
              "             'value': 1.2},\n",
              "            {'description': 'b, length normalization parameter',\n",
              "             'details': [],\n",
              "             'value': 0.75},\n",
              "            {'description': 'dl, length of field (approximate)',\n",
              "             'details': [],\n",
              "             'value': 264.0},\n",
              "            {'description': 'avgdl, average length of field',\n",
              "             'details': [],\n",
              "             'value': 142.53023}],\n",
              "           'value': 0.75310516}],\n",
              "         'value': 5.7716374}],\n",
              "       'value': 5.7716374}],\n",
              "     'value': 19.931646},\n",
              "    '_id': '254549',\n",
              "    '_index': 'stackoverflow',\n",
              "    '_node': 'g-38iH5tSgC9ryRIIQN5kg',\n",
              "    '_score': 19.931646,\n",
              "    '_shard': '[stackoverflow][0]',\n",
              "    '_source': {'id': 40554193,\n",
              "     'mappings': {'examplecase': {'properties': {'tbl_id': {'type': 'keyword'},\n",
              "        'texts': {'type': 'text'}}}},\n",
              "     'settings': {'number_of_replicas': 1, 'number_of_shards': 5},\n",
              "     'text_entry': 'trying write pair rdd elastic search elastic cloud version using elasticsearch spark plugin write es code using write es def predict imgs r import json pid r pid stuff r stuff return r pid json dumps res res map predict imgs es write conf es nodes image es es port es resource index type es nodes wan true es write operation upsert es mapping id product id es nodes discovery false es net http auth user username es net http auth pass pass es input json true es http timeout es scroll size es batch size bytes mb es http retries es batch size entries es batch write refresh false es batch write retry count es batch write retry wait res saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es write conf error get follows py jjavaerror error occurred calling z org apache spark api python pythonrdd saveasnewapihadoopfile org apache spark sparkexception job aborted due stage failure task stage failed times recent failure lost task stage tid org apache spark sparkexception python worker exited unexpectedly crashed interesting part works take first elements rdd make new rdd write es works flawlessly x sc parallelize res take x saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es write conf using elastic cloud cloud offering elastic search databricks cloud offering apache spark could es able keep put spark writing es increased elastic cloud size gb ram gb ram recommended configs es write conf used confs think updating es help help appreciated struggling days thank'},\n",
              "    '_type': 'Blog'}],\n",
              "  'max_score': 25.39941,\n",
              "  'total': {'relation': 'gte', 'value': 10000}},\n",
              " 'timed_out': False,\n",
              " 'took': 25}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61mRFLxtnh0I",
        "outputId": "05f5dd08-36a8-458a-cb4c-5aad9620ea96"
      },
      "source": [
        "#Extract _score of the first result\n",
        "result['hits']['hits'][0]['_explanation']['details'][0]['value']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.732946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYLxmR9KoMDN",
        "outputId": "c0638c30-b186-4b0e-caea-ce14edd02c6c"
      },
      "source": [
        "#Extract boost component of the first result\n",
        "result['hits']['hits'][0]['_explanation']['details'][0]['details']\\\n",
        "  [0]['details'][0]['value']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRIM68dXoUZC",
        "outputId": "1f5d16ee-80ee-4b5d-cba3-1310ca362c55"
      },
      "source": [
        "#Extract idf component of the first result\n",
        "result['hits']['hits'][0]['_explanation']['details'][0]['details']\\\n",
        "  [0]['details'][1]['value']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.045457"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SThrXj6MoYBn",
        "outputId": "5913d66a-895d-4c1b-ce3c-a665c7205310"
      },
      "source": [
        "#Extract tf component of the first result\n",
        "result['hits']['hits'][0]['_explanation']['details'][0]['details']\\\n",
        "  [0]['details'][2]['value']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80698806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHhjRmGUobXV",
        "outputId": "bb10e202-1465-4926-c116-29b198d60d9c"
      },
      "source": [
        "#Checking the formula consistency for 10 query found from query_word\n",
        "\n",
        "print(\"Similarity Analysis for the phrase'\", query_word,\"'\")\n",
        "\n",
        "for i in range(len(result['hits']['hits'])):\n",
        "    \n",
        "    score = result['hits']['hits'][i]['_explanation']['details'][0]\\\n",
        "            ['value']\n",
        "    boost = result['hits']['hits'][i]['_explanation']['details'][0]\\\n",
        "            ['details'][0]['details'][0]['value'] \n",
        "    idf  =  result['hits']['hits'][i]['_explanation']['details'][0]\\\n",
        "            ['details'][0]['details'][1]['value'] \n",
        "    tf   =  result['hits']['hits'][i]['_explanation']['details'][0]\\\n",
        "            ['details'][0]['details'][2]['value']\n",
        "\n",
        "    text =  result['hits']['hits'][i]['_source']['text_entry']\n",
        "    \n",
        "    print('document id = ',result['hits']['hits'][i]['_id'])\n",
        "    print('score = ',score, '=', boost, '*', idf, '*', tf, '--> error=',\\\n",
        "          boost*idf*tf-score)\n",
        "    print('document content: ', result['hits']['hits'][i]['_source']['text_entry'],'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity Analysis for the phrase' elasticsearch instance databricks spark '\n",
            "document id =  42157\n",
            "score =  10.732946 = 2.2 * 6.045457 * 0.80698806 --> error= -4.4426447409762204e-07\n",
            "document content:  trying index data elasticsearch databricks using pyspark works configuration scala trying write es using pyspark fails needed based transforms using python packages esconf esconf es nodes wan true esconf es port esconf es net ssl true esconf es read metadata true esconf es nodes esurl esconf ex index auto create true esconf es resource auto create geohashing testing geocordsschema write format org elasticsearch spark sql options esconf mode overwrite save geohash test error produced org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting apos es nodes wan apos ideas suggestions missing config would work scala using pyspark \n",
            "\n",
            "document id =  278687\n",
            "score =  11.388262 = 2.2 * 6.045457 * 0.8562599 --> error= -7.061965376919943e-07\n",
            "document content:  figure write dataframe elasticsearch using python spark followed steps code read file df sqlcontext read format com databricks spark csv options header true load vagrant data input input csv schema customschema df registertemptable data kpis kpi sqlcontext sql select data es conf es nodes es port es resource kpi kpi rdd saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es conf code gives caused net razorvine pickle pickleexception expected zero arguments construction classdict pyspark sql types create row also started script spark submit master spark aggregator jars jars elasticsearch hadoop dist elasticsearch hadoop jar vagrant scripts aggregation py ensure elasticsearch hadoop loaded \n",
            "\n",
            "document id =  56359\n",
            "score =  11.271469 = 2.2 * 6.045457 * 0.8474785 --> error= -3.73616098059415e-07\n",
            "document content:  running spark inside glue write aws elasticsearch following configuration spark conf set es nodes nodes indexname conf set es port conf set es batch write retry count conf set es batch size bytes kb conf set es batch size entries conf set es index auto create false conf set es nodes wan true conf set es net ssl true however get following error diagnostics user class threw exception org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan org elasticsearch hadoop rest initializationutils discoverclusterinfo initializationutils java org elasticsearch spark rdd esspark dosavetoes esspark scala know vpc running elasticsearch instance sure set glue spark different problem idea also tried add glue jdbc connection use proper vpc connection sure set properly import scala reflect runtime universe def savetoes lt product typetag index string data rdd sparkprovider gluecontext getjdbcsink catalogconnection elasticsearch connection options jsonoptions transformationcontext sinktoelasticsearch writedynamicframe dynamicframe sparkprovider sqlcontext createdataframe data sparkprovider gluecontext \n",
            "\n",
            "document id =  11139\n",
            "score =  5.566202 = 2.2 * 3.3369405 * 0.758207 --> error= -3.7949630016242963e-07\n",
            "document content:  new big data infrastructure need refactor web service actually done python querying google bigquery aspnet mvc core querying azure databricks would like run sql query spark instance azure databricks actual implementation using pyspark sparksession found implementation library net net apache spark microsoft spark feel like app using must run close spark instance need install java proper spark instance really feel missing something expecting something similar database call connexion string api accessed web site web site allows user build scenario defining couple attributes run query using attribute call web api api return sql result data frame translated model could align right technologies much going azure big data thank much help \n",
            "\n",
            "document id =  312047\n",
            "score =  3.4006798 = 2.2 * 3.3369405 * 0.46322775 --> error= -2.326624750637052e-07\n",
            "document content:  spark cluster use local mode want read csv databricks external library spark csv start app follows import os import sys os environ spark home home mebuddy programs spark bin hadoop spark home os environ get spark home none sys path insert spark home python sys path insert os path join spark home python lib py j src zip pyspark import sparkcontext sparkconf sqlcontext try sc except nameerror print initializing sparkcontext sc sparkcontext sq sqlcontext sc df sq read format com databricks spark csv options header true inferschema true load path file csv run get following error java lang classnotfoundexception failed load class data source com databricks spark csv question load databricks spark csv library inside python code want load outside using packages instance tried add following lines work os environ spark classpath home mebuddy programs spark lib spark csv jar \n",
            "\n",
            "document id =  20885\n",
            "score =  12.0147915 = 2.2 * 6.045457 * 0.9033674 --> error= -2.0181603765934142e-07\n",
            "document content:  trying read data elasticsearch via spark scala see lot post addressing question tried options mentioned various posts seems nothing working jar used elasticsearch hadoop jar used elasticsearch spark jar without success elastic search version spark scala code import org apache spark sparkconf import org apache spark sql sparksession import org apache spark sparkcontext import org apache spark sparkcontext import org elasticsearch spark val spark sparksession builder appname elasticspark master local getorcreate val reader spark read format org elasticsearch spark sql option es index auto create true option spark serializer org apache spark serializer kryoserializer option es port option es nodes xxxxxxxxx option es nodes wan true option es net http auth user xxxxxx option es net http auth pass xxxxxxxx val read reader load index type error error rest networkclient node xxxxxxxxx failed server xxxxxxxxxxxxx failed respond nodes left aborting org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan org elasticsearch hadoop rest initializationutils discoveresversion initializationutils java org elasticsearch spark sql schemautils discovermappingandgeofields schemautils scala org elasticsearch spark sql schemautils discovermapping schemautils scala org elasticsearch spark sql elasticsearchrelation lazyschema lzycompute defaultsource scala org elasticsearch spark sql elasticsearchrelation lazyschema defaultsource scala org elasticsearch spark sql elasticsearchrelation anonfun schema apply defaultsource scala org elasticsearch spark sql elasticsearchrelation anonfun schema apply defaultsource scala scala option getorelse option scala org elasticsearch spark sql elasticsearchrelation schema defaultsource scala org apache spark sql execution datasources datasource resolverelation datasource scala org apache spark sql dataframereader loadv source dataframereader scala org apache spark sql dataframereader load dataframereader scala org apache spark sql dataframereader load dataframereader scala elided caused org elasticsearch hadoop rest eshadoopnonodesleftexception connection error check network proxy settings nodes failed tried xxxxxxxxxxx org elasticsearch hadoop rest networkclient execute networkclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient execute restclient java org elasticsearch hadoop rest restclient get restclient java org elasticsearch hadoop rest restclient remoteesversion restclient java org elasticsearch hadoop rest initializationutils discoveresversion initializationutils java apart also tried properties without success option es net ssl cert allow self signed true option es net ssl truststore location lt path elasticsearch cert file gt option es net ssl truststore pass xxxxxx please note elasticsearch node within unix edge node http xxxxxx mentioning case makes difference code missing properties please help \n",
            "\n",
            "document id =  110573\n",
            "score =  11.658236 = 2.2 * 6.045457 * 0.87655866 --> error= -1.0885832342211188e-06\n",
            "document content:  trying loading data pyspark jupyteer notebook elasticsearch gives error error occurred calling z org apache spark api python pythonrdd saveasnewapihadoopfile org elasticsearch hadoop eshadoopillegalargumentexception detect es version typically happens network elasticsearch cluster accessible targeting wan cloud instance without proper setting es nodes wan conf x saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat get pyspark jar file valueclass org elasticsearch hadoop mr linkedmapwritable conf conf \n",
            "\n",
            "document id =  183631\n",
            "score =  5.4296827 = 2.2 * 3.3369405 * 0.73961085 --> error= -4.2087026486115064e-07\n",
            "document content:  use cloudera deploy zeppelin spark yarn hdfs cluster right one instance zeppelin spark execution spark notebooks affects every user instance stop spark context user notebook affects user notebooks seen option zeppelin isolate interpreters way provide user cluster demand maybe using docker building image zeppelin spark user limiting resources ones provided user cluster quite lost implement even possible ideal scenario would approach like databricks cluster resources isolated users \n",
            "\n",
            "document id =  80825\n",
            "score =  3.9561253 = 2.2 * 3.3369405 * 0.53888845 --> error= -1.7366810523000709e-07\n",
            "document content:  need connect spark redshift instance generate data using spark scala used compatible jdbc connector spark redshift connector facing weird problem using pyspark df sqlcontext read format com databricks spark redshift option query select top fact table option url jdbc redshift redshift host events user usernmae amp password pass option tempdir redshift archive load df show gives error permission denied bucket weird see files created bucket read ps set accesskey secret access key also ps also confused n file system connector used https github com databricks spark redshift tree branch x \n",
            "\n",
            "document id =  254549\n",
            "score =  9.544981 = 2.2 * 6.045457 * 0.71766746 --> error= 9.340428519521993e-08\n",
            "document content:  trying write pair rdd elastic search elastic cloud version using elasticsearch spark plugin write es code using write es def predict imgs r import json pid r pid stuff r stuff return r pid json dumps res res map predict imgs es write conf es nodes image es es port es resource index type es nodes wan true es write operation upsert es mapping id product id es nodes discovery false es net http auth user username es net http auth pass pass es input json true es http timeout es scroll size es batch size bytes mb es http retries es batch size entries es batch write refresh false es batch write retry count es batch write retry wait res saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es write conf error get follows py jjavaerror error occurred calling z org apache spark api python pythonrdd saveasnewapihadoopfile org apache spark sparkexception job aborted due stage failure task stage failed times recent failure lost task stage tid org apache spark sparkexception python worker exited unexpectedly crashed interesting part works take first elements rdd make new rdd write es works flawlessly x sc parallelize res take x saveasnewapihadoopfile path outputformatclass org elasticsearch hadoop mr esoutputformat keyclass org apache hadoop io nullwritable valueclass org elasticsearch hadoop mr linkedmapwritable conf es write conf using elastic cloud cloud offering elastic search databricks cloud offering apache spark could es able keep put spark writing es increased elastic cloud size gb ram gb ram recommended configs es write conf used confs think updating es help help appreciated struggling days thank \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0AUowjRtSH"
      },
      "source": [
        "Check below link for futher module similarity with classic scoring\n",
        "https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html"
      ]
    }
  ]
}